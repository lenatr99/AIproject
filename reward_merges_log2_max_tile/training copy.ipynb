{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.24.1\n",
      "PyTorch version: 2.0.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import agent\n",
    "import numpy as np\n",
    "from game import Game\n",
    "from collections import deque\n",
    "from agent import Agent\n",
    "import time\n",
    "import torch\n",
    "import datetime\n",
    "import random\n",
    "import model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "import math\n",
    "import os\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_state(state, mode='plain'):\n",
    "    \"\"\" Returns the (log2 / 17) of the values in the state array \"\"\"\n",
    "    \n",
    "    if mode == 'plain':\n",
    "        return np.reshape(state, -1)\n",
    "    \n",
    "    elif mode == 'plain_hw':\n",
    "        return np.concatenate([np.reshape(state, -1), np.reshape(np.transpose(state), -1)])\n",
    "    \n",
    "    elif mode == 'log2':\n",
    "        state = np.reshape(state, -1)\n",
    "        state[state == 0] = 1\n",
    "        return np.log2(state) / 17\n",
    "    \n",
    "    elif mode == 'one_hot':\n",
    "        \n",
    "        state = np.reshape(state, -1)\n",
    "        state[state == 0] = 1\n",
    "        state = np.log2(state)\n",
    "        state = state.astype(int)\n",
    "        new_state = np.reshape(np.eye(18)[state], -1)     \n",
    "        return new_state\n",
    "    \n",
    "    elif mode == 'conv':\n",
    "        X = np.reshape(state, (4,4))\n",
    "        print(\"X\")\n",
    "        power_mat = np.zeros(shape=(1,4,4,16),dtype=np.float32)\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if(X[i][j]==0):\n",
    "                    power_mat[0][i][j][0] = 1.0\n",
    "                else:\n",
    "                    power = int(math.log(X[i][j],2))\n",
    "                    power_mat[0][i][j][power] = 1.0\n",
    "\n",
    "        return power_mat \n",
    "\n",
    "    else:\n",
    "        return state\n",
    "\n",
    "def dqn(n_episodes=100, eps_start=0.05, eps_end=0.001, eps_decay=0.995, step_penalty = 0, sample_mode = 'error',\n",
    "        start_learn_iterations = 20, bootstrap_iterations = 0, bootstrap_every=50):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        step_penalty (int): if we want to deduct some points from the reward for taking a step, put it here\n",
    "        sample_mode (str): how to assign sample probabilities for experiences in the replay buffer\n",
    "        learn_iterations (int): number of learning iterations after each episode\n",
    "        bootstrap_iterations (int): how many bootstrapping iterations to perform*\n",
    "        bootstrap_every (int): how often to perform bootstrapping\n",
    "        \n",
    "        * bootstrapping here - episodes starting with randomly filled intermediate state boards, imitating the situation\n",
    "        when the game started from the middle.\n",
    "    \"\"\"\n",
    "    \n",
    "    eps = eps_start\n",
    "    starting_iteration = agent.current_iteration\n",
    "    best_game_history, worst_game_history = load_game_history(version)\n",
    "    learn_iterations = start_learn_iterations - (agent.current_iteration // 5000) if agent.current_iteration < start_learn_iterations * 5000 else 1\n",
    "    learn_iterations = start_learn_iterations\n",
    "    \n",
    "    # main loop\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        agent.current_iteration = agent.current_iteration + 1\n",
    "        time_start = time.time()\n",
    "        \n",
    "        # keep track of different actions taken per episode\n",
    "        actions = np.array([0, 0, 0, 0])\n",
    "        \n",
    "        # Starting with a fresh board\n",
    "        env.reset(2, step_penalty)                        # reset environment\n",
    "        \n",
    "        state = transform_state(env.current_state(), mode = 'plain')   # get the current state        \n",
    "        reward = env.reward                               # get the reward\n",
    "        total_rewards = reward                            # initialize total rewards\n",
    "        score = env.score                                 # initialize the score\n",
    "        agent.total_steps = 0\n",
    "        \n",
    "        # keep playing\n",
    "        while not env.done:\n",
    "            reward = env.negative_reward\n",
    "\n",
    "            # state_1 = transform_state(state, mode = 'conv')             \n",
    "            action_values = agent.act(state)    # select an action\n",
    "            \n",
    "            actions_sorted = [(i, v) for i, v in enumerate(action_values[0])]\n",
    "            actions_sorted = sorted(actions_sorted, key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            random_action = random.choice(np.arange(agent.action_size))\n",
    "            action_idx = 0\n",
    "            env.moved = False\n",
    "            while not env.moved:\n",
    "                \n",
    "                if random.random() < eps:\n",
    "                    action_elem = actions_sorted[random_action]\n",
    "                else:\n",
    "                    action_elem = actions_sorted[action_idx]\n",
    "                    action_idx += 1\n",
    "\n",
    "                action = np.int64(action_elem[0])\n",
    "                actions[action] += 1\n",
    "                env.step(action, action_values)                              # send the action to environment\n",
    "                next_state = transform_state(env.current_state(), mode = 'plain')   # get the current state       \n",
    "                \n",
    "                reward = env.reward                             # get the reward\n",
    "\n",
    "                # save the results of the step\n",
    "\n",
    "                error = np.abs(reward - action_elem[1]) ** 2\n",
    "                score = env.score\n",
    "                done = env.done                             # check if episode has finished\n",
    "\n",
    "                # learning step\n",
    "                if len(agent.actions_avg_list) > 0:\n",
    "                    actions_dist = [np.mean(agent.actions_deque[i]) for i in range(4)][action]\n",
    "                else:\n",
    "                    actions_dist = (actions / np.sum(actions))[action]\n",
    "\n",
    "                # Save this experience\n",
    "                # print(\"reward: \", reward)\n",
    "                agent.step(state, action, reward, next_state, done, error, actions_dist) \n",
    "\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                agent.total_steps += 1\n",
    "                total_rewards += reward\n",
    "\n",
    "                if done:\n",
    "                    break \n",
    "        \n",
    "        # Do the actual learning\n",
    "        agent.learn(learn_iterations, mode=sample_mode, save_loss = True, weight = env.game_board.max())\n",
    "        \n",
    "        # Calculate action stats\n",
    "        actions = actions / env.steps\n",
    "        \n",
    "        agent.actions_deque[0].append(actions[0])\n",
    "        agent.actions_deque[1].append(actions[1])\n",
    "        agent.actions_deque[2].append(actions[2])\n",
    "        agent.actions_deque[3].append(actions[3])\n",
    "        \n",
    "        agent.actions_avg_list.append([np.mean(agent.actions_deque[i]) for i in range(4)])\n",
    "        \n",
    "        # Here we keep track of the learning progress and save the best values\n",
    "        if total_rewards > agent.max_total_reward:\n",
    "            agent.max_total_reward = total_rewards\n",
    "        \n",
    "        if score > agent.max_score:\n",
    "            agent.max_score = score\n",
    "            agent.best_score_board = env.game_board.copy()\n",
    "            best_game_history = env.history.copy()\n",
    "        \n",
    "        if score < agent.min_score:\n",
    "            agent.min_score = score\n",
    "            worst_game_history = env.history.copy()\n",
    "        \n",
    "        if env.game_board.max() > agent.max_val:\n",
    "            agent.max_val = env.game_board.max()\n",
    "            agent.best_val_board = env.game_board.copy()\n",
    "        \n",
    "        if env.steps > agent.max_steps:\n",
    "            agent.max_steps = env.steps\n",
    "            agent.best_steps_board = env.game_board.copy()\n",
    "            \n",
    "        \n",
    "        agent.total_rewards_list.append(total_rewards)\n",
    "        agent.scores_list.append(score)                 # save most recent score to total\n",
    "        agent.max_vals_list.append(env.game_board.max())\n",
    "        agent.max_steps_list.append(env.steps)\n",
    "        \n",
    "        agent.last_n_scores.append(score)\n",
    "        agent.last_n_steps.append(env.steps)\n",
    "        agent.last_n_vals.append(env.game_board.max())\n",
    "        agent.last_n_total_rewards.append(total_rewards)\n",
    "        \n",
    "        agent.mean_scores.append(np.mean(agent.last_n_scores))\n",
    "        agent.mean_steps.append(np.mean(agent.last_n_steps))\n",
    "        agent.mean_vals.append(np.mean(agent.last_n_vals))\n",
    "        agent.mean_total_rewards.append(np.mean(agent.last_n_total_rewards))\n",
    "        \n",
    "        time_end = time.time()\n",
    "        \n",
    "        # ---------------------------\n",
    "        # BEGIN OF BOOTSTRAPPING PART\n",
    "        # ---------------------------\n",
    "        if i_episode % bootstrap_every == 0:\n",
    "            \n",
    "            for b_episode in range(bootstrap_iterations):\n",
    "                env.reset(2, step_penalty, bootstrapping=True)  # reset environment\n",
    "                state = transform_state(env.current_state(), mode = 'plain')   # get the current state        \n",
    "                reward = env.reward\n",
    "                score = env.score\n",
    "                first_step = True\n",
    "                \n",
    "                # keep playing while there are available moves\n",
    "                while not env.done:\n",
    "                    reward = env.negative_reward\n",
    "            \n",
    "                    action_values = agent.act(state)    # select an action\n",
    "\n",
    "                    actions_sorted = [(i, v) for i, v in enumerate(action_values[0])]\n",
    "                    actions_sorted = sorted(actions_sorted, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                    random_action = random.choice(np.arange(agent.action_size))\n",
    "                    action_idx = 0\n",
    "                    while reward == env.negative_reward:\n",
    "                        \n",
    "                        if random.random() < eps:\n",
    "                            action_elem = actions_sorted[random_action]\n",
    "                        else:\n",
    "                            try:\n",
    "                                action_elem = actions_sorted[action_idx]\n",
    "                                action_idx += 1\n",
    "                            except IndexError:\n",
    "                                print(action_idx)\n",
    "                                \n",
    "                                print(actions_sorted)\n",
    "                                return\n",
    "\n",
    "                        action = np.int64(action_elem[0])\n",
    "                        actions[action] += 1\n",
    "                        env.step(action, action_values)                                # send the action to environment\n",
    "                        next_state = transform_state(env.current_state(), mode = 'plain')   # get the current state        \n",
    "                        reward = env.reward                             # get the reward\n",
    "\n",
    "                        # save the results of the step\n",
    "\n",
    "                        error = np.abs(reward - action_elem[1]) ** 2\n",
    "                        score = env.score\n",
    "                        done = env.done                             # check if episode has finished\n",
    "\n",
    "                        # learning step\n",
    "                        if len(agent.actions_avg_list) > 0:\n",
    "                            actions_dist = [np.mean(agent.actions_deque[i]) for i in range(4)][action]\n",
    "                        else:\n",
    "                            actions_dist = (actions / np.sum(actions))[action]\n",
    "\n",
    "                        # Save this experience\n",
    "                        agent.step(state, action, reward, next_state, done, error, actions_dist) \n",
    "\n",
    "\n",
    "                        state = next_state\n",
    "\n",
    "                        agent.total_steps += 1\n",
    "                        total_rewards += reward\n",
    "\n",
    "                        if done:\n",
    "                            break \n",
    "                \n",
    "                # Perform learning\n",
    "                agent.learn(learn_iterations, mode=sample_mode, save_loss = False, weight = env.game_board.max())\n",
    "        \n",
    "        # ---------------------------\n",
    "        # END OF BOOTSTRAPPING PART\n",
    "        # ---------------------------\n",
    "        \n",
    "        # Increasing the epsilon every N episodes in order to allow for some exploration \n",
    "        if agent.current_iteration % 5000 == 0:\n",
    "            eps = eps * 2\n",
    "        else:\n",
    "            eps = max(eps_end, eps_decay*eps)    # decrease epsilon\n",
    "        \n",
    "        # Display training stats \n",
    "        if agent.current_iteration % 100 == 0:\n",
    "            clear_output()\n",
    "            \n",
    "            # Training metrics\n",
    "            fig, ax1 = plt.subplots()\n",
    "            fig.set_size_inches(16,6)\n",
    "            ax1.plot(agent.max_vals_list + [None for i in range(10000 - len(agent.scores_list))], label='Max cell value seen on board', alpha = 0.3, color='g')\n",
    "            ax1.plot(agent.mean_steps + [None for i in range(10000 - len(agent.scores_list))], label='Mean steps over last 50 episodes', color='b')            \n",
    "#             ax1.plot(agent.scores_list + [None for i in range(10000 - len(agent.scores_list))], label='Scores', color='c', alpha=0.2)            \n",
    "#             ax1.plot(agent.losses, label='Loss', color='b')\n",
    "#             ax1.set_yscale('log')\n",
    "            \n",
    "#             ax1.plot(agent.mean_scores + [None for i in range(10000 - len(agent.scores_list))], label='Mean score over last 50 episodes', color='b')\n",
    "\n",
    "            ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "            \n",
    "            ax2.plot(agent.mean_total_rewards + [None for i in range(10000 - len(agent.scores_list))], label='Mean total rewards over last 50 episodes', color='r')            \n",
    "            fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "            plt.xlabel('Episode #')\n",
    "            handles, labels = [(a + b) for a, b in zip(ax1.get_legend_handles_labels(), ax2.get_legend_handles_labels())]\n",
    "            plt.legend(handles, labels)\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure(figsize=(16,6))\n",
    "            plt.title('Loss')\n",
    "            plt.plot(agent.losses)\n",
    "            plt.yscale('log')\n",
    "            plt.show()\n",
    "            \n",
    "            # Averaged actions stats\n",
    "            plt.figure(figsize=(16,6))\n",
    "            plt.title('Averaged actions distribution per game')\n",
    "            a_list = np.array(agent.actions_avg_list).T\n",
    "            \n",
    "            plt.stackplot([i for i in range(1, len(agent.actions_avg_list)+1)], a_list[0], a_list[1], a_list[2], a_list[3], labels=['Up %0.2f' % (agent.actions_avg_list[-1][0] * 100), 'Down %0.2f' % (agent.actions_avg_list[-1][1] * 100), 'Left %0.2f' % (agent.actions_avg_list[-1][2] * 100), 'Right %0.2f' % (agent.actions_avg_list[-1][3] * 100)] )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "                \n",
    "            \n",
    "            # Display the board with the best score\n",
    "            env.draw_board(agent.best_score_board, 'Best score board')\n",
    "            \n",
    "            # Save the model and the game history\n",
    "            save_state(version, eps)\n",
    "            save_game_history(version, best_game_history, worst_game_history)\n",
    "            \n",
    "        s = '%d/%d | %0.2fs | Sc:%d | AvgSc:%d | TR:%.1f | AvgTR:%.1f | GlMaxVal:%d | Epsilon:%.4f' %\\\n",
    "              (agent.current_iteration, starting_iteration + n_episodes, time_end - time_start, score, np.mean(agent.last_n_scores), total_rewards, np.mean(agent.last_n_total_rewards), np.max(agent.max_vals_list), eps)\n",
    "        \n",
    "        s = s + ' ' * (120-len(s))\n",
    "        print(s, end='\\r')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './data/'\n",
    "\n",
    "def save_state(name, eps):\n",
    "    with open(base_dir+'/game_%s.pkl' % name, 'wb') as f:\n",
    "        state = {\n",
    "            'env': env,\n",
    "            'last_eps': eps\n",
    "        }\n",
    "        pickle.dump(state, f)\n",
    "    agent.save(name)\n",
    "    \n",
    "\n",
    "def save_game_history(name, best_history, worst_history):\n",
    "    with open(base_dir+'/best_game_history_%s.pkl' % name, 'wb') as f:\n",
    "        pickle.dump(best_history, f)\n",
    "    with open(base_dir+'/worst_game_history_%s.pkl' % name, 'wb') as f:\n",
    "        pickle.dump(worst_history, f)\n",
    "    \n",
    "def load_game_history(name):\n",
    "    best_history = []\n",
    "    worst_history = []\n",
    "    if os.path.exists(base_dir+'/best_game_history_%s.pkl' % name):\n",
    "        with open(base_dir+'/best_game_history_%s.pkl' % name, 'rb') as f:\n",
    "            best_history = pickle.load(f)\n",
    "    \n",
    "    if os.path.exists(base_dir+'/worst_game_history_%s.pkl' % name):\n",
    "        with open(base_dir+'/worst_game_history_%s.pkl' % name, 'rb') as f:\n",
    "            worst_history = pickle.load(f)\n",
    "    \n",
    "    return best_history, worst_history\n",
    "    \n",
    "def load_state(name):\n",
    "    with open(base_dir+'/game_%s.pkl' % name, 'rb') as f:\n",
    "        state = pickle.load(f)\n",
    "        g = state['env']\n",
    "        eps = state['last_eps']\n",
    "    a = Agent(state_size=4 * 4 * 18, fc1_units=1024, fc2_units=1024, fc3_units = 1024)\n",
    "    a.load(name)\n",
    "    return g, a, eps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization & training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'ohe_2_steps_move_penalty_log_reward_512_3_layers_random'\n",
    "# env, agent, eps = load_state(version)\n",
    "\n",
    "# Create the environment with 4x4 board\n",
    "env = Game(4, reward_mode='log2', negative_reward = 0, cell_move_penalty = 0)\n",
    "eps = 0.7\n",
    "# Create the agent, duplicating default values for visibility\n",
    "agent = Agent(state_size=4*4, action_size=env.action_size,\n",
    "              seed=42, fc1_units=1024, fc2_units=1024, fc3_units = 1024,\n",
    "              buffer_size = 100000, batch_size = 1024, lr = 0.004, use_expected_rewards=True, predict_steps = 2,\n",
    "             gamma = 0., tau = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/100000 | 6.05s | Sc:264 | AvgSc:267 | TR:-11 | AvgTR:-12 | GlMaxVal:128 | Epsilon:0.6944                              \r"
     ]
    }
   ],
   "source": [
    "# Run the training\n",
    "dqn(n_episodes=100000, \n",
    "    eps_start=eps or 0.05, \n",
    "    eps_end=0.00001, \n",
    "    eps_decay=0.999, \n",
    "    step_penalty = 0, \n",
    "    sample_mode = 'random',\n",
    "    start_learn_iterations = 10, \n",
    "    bootstrap_iterations = 0, \n",
    "    bootstrap_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
