{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.24.3\n",
      "PyTorch version: 2.0.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import agent\n",
    "import numpy as np\n",
    "from game import Game\n",
    "from collections import deque\n",
    "from agent import Agent\n",
    "import time\n",
    "import torch\n",
    "import datetime\n",
    "import random\n",
    "import model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "import math\n",
    "import os\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_state(state, mode='plain'):\n",
    "    \"\"\" Returns the (log2 / 17) of the values in the state array \"\"\"\n",
    "    \n",
    "    if mode == 'plain':\n",
    "        return np.reshape(state, -1)\n",
    "    \n",
    "    elif mode == 'plain_hw':\n",
    "        return np.concatenate([np.reshape(state, -1), np.reshape(np.transpose(state), -1)])\n",
    "    \n",
    "    elif mode == 'log2':\n",
    "        state = np.reshape(state, -1)\n",
    "        state[state == 0] = 1\n",
    "        return np.log2(state) / 17\n",
    "    \n",
    "    elif mode == 'one_hot':\n",
    "        \n",
    "        state = np.reshape(state, -1)\n",
    "        state[state == 0] = 1\n",
    "        state = np.log2(state)\n",
    "        state = state.astype(int)\n",
    "        new_state = np.reshape(np.eye(18)[state], -1)     \n",
    "        return new_state\n",
    "    \n",
    "    elif mode == 'conv':\n",
    "        X = np.reshape(state, (4,4))\n",
    "        print(\"X\")\n",
    "        power_mat = np.zeros(shape=(1,4,4,16),dtype=np.float32)\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if(X[i][j]==0):\n",
    "                    power_mat[0][i][j][0] = 1.0\n",
    "                else:\n",
    "                    power = int(math.log(X[i][j],2))\n",
    "                    power_mat[0][i][j][power] = 1.0\n",
    "\n",
    "        return power_mat \n",
    "\n",
    "    else:\n",
    "        return state\n",
    "\n",
    "def dqn(n_episodes=100, eps_start=0.05, eps_end=0.001, eps_decay=0.995, step_penalty = 0, sample_mode = 'error',\n",
    "        start_learn_iterations = 20, bootstrap_iterations = 0, bootstrap_every=50):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        step_penalty (int): if we want to deduct some points from the reward for taking a step, put it here\n",
    "        sample_mode (str): how to assign sample probabilities for experiences in the replay buffer\n",
    "        learn_iterations (int): number of learning iterations after each episode\n",
    "        bootstrap_iterations (int): how many bootstrapping iterations to perform*\n",
    "        bootstrap_every (int): how often to perform bootstrapping\n",
    "        \n",
    "        * bootstrapping here - episodes starting with randomly filled intermediate state boards, imitating the situation\n",
    "        when the game started from the middle.\n",
    "    \"\"\"\n",
    "    \n",
    "    eps = eps_start\n",
    "    starting_iteration = agent.current_iteration\n",
    "    best_game_history, worst_game_history = load_game_history(version)\n",
    "    learn_iterations = start_learn_iterations - (agent.current_iteration // 5000) if agent.current_iteration < start_learn_iterations * 5000 else 1\n",
    "    learn_iterations = start_learn_iterations\n",
    "    \n",
    "    # main loop\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        agent.current_iteration = agent.current_iteration + 1\n",
    "        time_start = time.time()\n",
    "        \n",
    "        # keep track of different actions taken per episode\n",
    "        actions = np.array([0, 0, 0, 0])\n",
    "        \n",
    "        # Starting with a fresh board\n",
    "        env.reset(2, step_penalty)                        # reset environment\n",
    "        \n",
    "        state = transform_state(env.current_state(), mode = 'plain')   # get the current state        \n",
    "        reward = env.reward                               # get the reward\n",
    "        total_rewards = reward                            # initialize total rewards\n",
    "        score = env.score                                 # initialize the score\n",
    "        agent.total_steps = 0\n",
    "        \n",
    "        # keep playing\n",
    "        while not env.done:\n",
    "            reward = env.negative_reward\n",
    "\n",
    "            # state_1 = transform_state(state, mode = 'conv')             \n",
    "            action_values = agent.act(state)    # select an action\n",
    "            \n",
    "            actions_sorted = [(i, v) for i, v in enumerate(action_values[0])]\n",
    "            actions_sorted = sorted(actions_sorted, key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            random_action = random.choice(np.arange(agent.action_size))\n",
    "            action_idx = 0\n",
    "            env.moved = False\n",
    "            while not env.moved:\n",
    "                \n",
    "                if random.random() < eps:\n",
    "                    action_elem = actions_sorted[random_action]\n",
    "                else:\n",
    "                    action_elem = actions_sorted[action_idx]\n",
    "                    action_idx += 1\n",
    "\n",
    "                action = np.int64(action_elem[0])\n",
    "                actions[action] += 1\n",
    "                env.step(action, action_values)                              # send the action to environment\n",
    "                next_state = transform_state(env.current_state(), mode = 'plain')   # get the current state       \n",
    "                \n",
    "                reward = env.reward                             # get the reward\n",
    "\n",
    "                # save the results of the step\n",
    "\n",
    "                error = np.abs(reward - action_elem[1]) ** 2\n",
    "                score = env.score\n",
    "                done = env.done                             # check if episode has finished\n",
    "\n",
    "                # learning step\n",
    "                if len(agent.actions_avg_list) > 0:\n",
    "                    actions_dist = [np.mean(agent.actions_deque[i]) for i in range(4)][action]\n",
    "                else:\n",
    "                    actions_dist = (actions / np.sum(actions))[action]\n",
    "\n",
    "                # Save this experience\n",
    "                # print(\"reward: \", reward)\n",
    "                agent.step(state, action, reward, next_state, done, error, actions_dist) \n",
    "\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                agent.total_steps += 1\n",
    "                total_rewards += reward\n",
    "\n",
    "                if done:\n",
    "                    break \n",
    "        \n",
    "        # Do the actual learning\n",
    "        agent.learn(learn_iterations, mode=sample_mode, save_loss = True, weight = env.game_board.max())\n",
    "        \n",
    "        # Calculate action stats\n",
    "        actions = actions / env.steps\n",
    "        \n",
    "        agent.actions_deque[0].append(actions[0])\n",
    "        agent.actions_deque[1].append(actions[1])\n",
    "        agent.actions_deque[2].append(actions[2])\n",
    "        agent.actions_deque[3].append(actions[3])\n",
    "        \n",
    "        agent.actions_avg_list.append([np.mean(agent.actions_deque[i]) for i in range(4)])\n",
    "        \n",
    "        # Here we keep track of the learning progress and save the best values\n",
    "        if total_rewards > agent.max_total_reward:\n",
    "            agent.max_total_reward = total_rewards\n",
    "        \n",
    "        if score > agent.max_score:\n",
    "            agent.max_score = score\n",
    "            agent.best_score_board = env.game_board.copy()\n",
    "            best_game_history = env.history.copy()\n",
    "        \n",
    "        if score < agent.min_score:\n",
    "            agent.min_score = score\n",
    "            worst_game_history = env.history.copy()\n",
    "        \n",
    "        if env.game_board.max() > agent.max_val:\n",
    "            agent.max_val = env.game_board.max()\n",
    "            agent.best_val_board = env.game_board.copy()\n",
    "        \n",
    "        if env.steps > agent.max_steps:\n",
    "            agent.max_steps = env.steps\n",
    "            agent.best_steps_board = env.game_board.copy()\n",
    "            \n",
    "        \n",
    "        agent.total_rewards_list.append(total_rewards)\n",
    "        agent.scores_list.append(score)                 # save most recent score to total\n",
    "        agent.max_vals_list.append(env.game_board.max())\n",
    "        agent.max_steps_list.append(env.steps)\n",
    "        \n",
    "        agent.last_n_scores.append(score)\n",
    "        agent.last_n_steps.append(env.steps)\n",
    "        agent.last_n_vals.append(env.game_board.max())\n",
    "        agent.last_n_total_rewards.append(total_rewards)\n",
    "        \n",
    "        agent.mean_scores.append(np.mean(agent.last_n_scores))\n",
    "        agent.mean_steps.append(np.mean(agent.last_n_steps))\n",
    "        agent.mean_vals.append(np.mean(agent.last_n_vals))\n",
    "        agent.mean_total_rewards.append(np.mean(agent.last_n_total_rewards))\n",
    "        \n",
    "        time_end = time.time()\n",
    "        \n",
    "        # ---------------------------\n",
    "        # BEGIN OF BOOTSTRAPPING PART\n",
    "        # ---------------------------\n",
    "        if i_episode % bootstrap_every == 0:\n",
    "            \n",
    "            for b_episode in range(bootstrap_iterations):\n",
    "                env.reset(2, step_penalty, bootstrapping=True)  # reset environment\n",
    "                state = transform_state(env.current_state(), mode = 'plain')   # get the current state        \n",
    "                reward = env.reward\n",
    "                score = env.score\n",
    "                first_step = True\n",
    "                \n",
    "                # keep playing while there are available moves\n",
    "                while not env.done:\n",
    "                    reward = env.negative_reward\n",
    "            \n",
    "                    action_values = agent.act(state)    # select an action\n",
    "\n",
    "                    actions_sorted = [(i, v) for i, v in enumerate(action_values[0])]\n",
    "                    actions_sorted = sorted(actions_sorted, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                    random_action = random.choice(np.arange(agent.action_size))\n",
    "                    action_idx = 0\n",
    "                    while reward == env.negative_reward:\n",
    "                        \n",
    "                        if random.random() < eps:\n",
    "                            action_elem = actions_sorted[random_action]\n",
    "                        else:\n",
    "                            try:\n",
    "                                action_elem = actions_sorted[action_idx]\n",
    "                                action_idx += 1\n",
    "                            except IndexError:\n",
    "                                print(action_idx)\n",
    "                                \n",
    "                                print(actions_sorted)\n",
    "                                return\n",
    "\n",
    "                        action = np.int64(action_elem[0])\n",
    "                        actions[action] += 1\n",
    "                        env.step(action, action_values)                                # send the action to environment\n",
    "                        next_state = transform_state(env.current_state(), mode = 'plain')   # get the current state        \n",
    "                        reward = env.reward                             # get the reward\n",
    "\n",
    "                        # save the results of the step\n",
    "\n",
    "                        error = np.abs(reward - action_elem[1]) ** 2\n",
    "                        score = env.score\n",
    "                        done = env.done                             # check if episode has finished\n",
    "\n",
    "                        # learning step\n",
    "                        if len(agent.actions_avg_list) > 0:\n",
    "                            actions_dist = [np.mean(agent.actions_deque[i]) for i in range(4)][action]\n",
    "                        else:\n",
    "                            actions_dist = (actions / np.sum(actions))[action]\n",
    "\n",
    "                        # Save this experience\n",
    "                        agent.step(state, action, reward, next_state, done, error, actions_dist) \n",
    "\n",
    "\n",
    "                        state = next_state\n",
    "\n",
    "                        agent.total_steps += 1\n",
    "                        total_rewards += reward\n",
    "\n",
    "                        if done:\n",
    "                            break \n",
    "                \n",
    "                # Perform learning\n",
    "                agent.learn(learn_iterations, mode=sample_mode, save_loss = False, weight = env.game_board.max())\n",
    "        \n",
    "        # ---------------------------\n",
    "        # END OF BOOTSTRAPPING PART\n",
    "        # ---------------------------\n",
    "        \n",
    "        # Increasing the epsilon every N episodes in order to allow for some exploration \n",
    "        if agent.current_iteration % 5000 == 0:\n",
    "            eps = eps * 2\n",
    "        else:\n",
    "            eps = max(eps_end, eps_decay*eps)    # decrease epsilon\n",
    "        \n",
    "        # Display training stats \n",
    "        if agent.current_iteration % 100 == 0:\n",
    "            clear_output()\n",
    "            \n",
    "            # Training metrics\n",
    "            fig, ax1 = plt.subplots()\n",
    "            fig.set_size_inches(16,6)\n",
    "            ax1.plot(agent.max_vals_list + [None for i in range(10000 - len(agent.scores_list))], label='Max cell value seen on board', alpha = 0.3, color='g')\n",
    "            ax1.plot(agent.mean_steps + [None for i in range(10000 - len(agent.scores_list))], label='Mean steps over last 50 episodes', color='b')            \n",
    "#             ax1.plot(agent.scores_list + [None for i in range(10000 - len(agent.scores_list))], label='Scores', color='c', alpha=0.2)            \n",
    "#             ax1.plot(agent.losses, label='Loss', color='b')\n",
    "#             ax1.set_yscale('log')\n",
    "            \n",
    "#             ax1.plot(agent.mean_scores + [None for i in range(10000 - len(agent.scores_list))], label='Mean score over last 50 episodes', color='b')\n",
    "\n",
    "            ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "            \n",
    "            ax2.plot(agent.mean_total_rewards + [None for i in range(10000 - len(agent.scores_list))], label='Mean total rewards over last 50 episodes', color='r')            \n",
    "            fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "            plt.xlabel('Episode #')\n",
    "            handles, labels = [(a + b) for a, b in zip(ax1.get_legend_handles_labels(), ax2.get_legend_handles_labels())]\n",
    "            plt.legend(handles, labels)\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure(figsize=(16,6))\n",
    "            plt.title('Loss')\n",
    "            plt.plot(agent.losses)\n",
    "            plt.yscale('log')\n",
    "            plt.show()\n",
    "            \n",
    "            # Averaged actions stats\n",
    "            plt.figure(figsize=(16,6))\n",
    "            plt.title('Averaged actions distribution per game')\n",
    "            a_list = np.array(agent.actions_avg_list).T\n",
    "            \n",
    "            plt.stackplot([i for i in range(1, len(agent.actions_avg_list)+1)], a_list[0], a_list[1], a_list[2], a_list[3], labels=['Up %0.2f' % (agent.actions_avg_list[-1][0] * 100), 'Down %0.2f' % (agent.actions_avg_list[-1][1] * 100), 'Left %0.2f' % (agent.actions_avg_list[-1][2] * 100), 'Right %0.2f' % (agent.actions_avg_list[-1][3] * 100)] )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "                \n",
    "            \n",
    "            # Display the board with the best score\n",
    "            env.draw_board(agent.best_score_board, 'Best score board')\n",
    "            \n",
    "            # Save the model and the game history\n",
    "            save_state(version, eps)\n",
    "            save_game_history(version, best_game_history, worst_game_history)\n",
    "            \n",
    "        s = '%d/%d | %0.2fs | Sc:%d | AvgSc:%d | TR:%.1f | AvgTR:%.1f | GlMaxVal:%d | Epsilon:%.4f' %\\\n",
    "              (agent.current_iteration, starting_iteration + n_episodes, time_end - time_start, score, np.mean(agent.last_n_scores), total_rewards, np.mean(agent.last_n_total_rewards), np.max(agent.max_vals_list), eps)\n",
    "        \n",
    "        s = s + ' ' * (120-len(s))\n",
    "        print(s, end='\\r')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './data/'\n",
    "\n",
    "def save_state(name, eps):\n",
    "    with open(base_dir+'/game_%s.pkl' % name, 'wb') as f:\n",
    "        state = {\n",
    "            'env': env,\n",
    "            'last_eps': eps\n",
    "        }\n",
    "        pickle.dump(state, f)\n",
    "    agent.save(name)\n",
    "    \n",
    "\n",
    "def save_game_history(name, best_history, worst_history):\n",
    "    with open(base_dir+'/best_game_history_%s.pkl' % name, 'wb') as f:\n",
    "        pickle.dump(best_history, f)\n",
    "    with open(base_dir+'/worst_game_history_%s.pkl' % name, 'wb') as f:\n",
    "        pickle.dump(worst_history, f)\n",
    "    \n",
    "def load_game_history(name):\n",
    "    best_history = []\n",
    "    worst_history = []\n",
    "    if os.path.exists(base_dir+'/best_game_history_%s.pkl' % name):\n",
    "        with open(base_dir+'/best_game_history_%s.pkl' % name, 'rb') as f:\n",
    "            best_history = pickle.load(f)\n",
    "    \n",
    "    if os.path.exists(base_dir+'/worst_game_history_%s.pkl' % name):\n",
    "        with open(base_dir+'/worst_game_history_%s.pkl' % name, 'rb') as f:\n",
    "            worst_history = pickle.load(f)\n",
    "    \n",
    "    return best_history, worst_history\n",
    "    \n",
    "def load_state(name):\n",
    "    with open(base_dir+'/game_%s.pkl' % name, 'rb') as f:\n",
    "        state = pickle.load(f)\n",
    "        g = state['env']\n",
    "        eps = state['last_eps']\n",
    "    a = Agent(state_size=4 * 4)\n",
    "    a.load(name)\n",
    "    return g, a, eps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization & training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'oh_merges_gama_dropout_gamma09_lr0005_eps05'\n",
    "# env, agent, eps = load_state(version)\n",
    "\n",
    "# Create the environment with 4x4 board\n",
    "env = Game(4, reward_mode='log2', negative_reward = 0, cell_move_penalty = 0)\n",
    "eps = 0.5\n",
    "# Create the agent, duplicating default values for visibility\n",
    "agent = Agent(state_size=4*4, action_size=env.action_size,\n",
    "              seed=42, buffer_size = 100000, batch_size = 512, lr = 0.005, use_expected_rewards=False, predict_steps = 2,\n",
    "             gamma = 0.9, tau = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/100000 | 5.31s | Sc:304 | AvgSc:273 | TR:13.0 | AvgTR:8.8 | GlMaxVal:128 | Epsilon:0.4930                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Run the training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dqn(n_episodes\u001b[39m=\u001b[39;49m\u001b[39m100000\u001b[39;49m, \n\u001b[0;32m      3\u001b[0m     eps_start\u001b[39m=\u001b[39;49meps \u001b[39mor\u001b[39;49;00m \u001b[39m0.05\u001b[39;49m, \n\u001b[0;32m      4\u001b[0m     eps_end\u001b[39m=\u001b[39;49m\u001b[39m0.00001\u001b[39;49m, \n\u001b[0;32m      5\u001b[0m     eps_decay\u001b[39m=\u001b[39;49m\u001b[39m0.999\u001b[39;49m,\n\u001b[0;32m      6\u001b[0m     step_penalty \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m, \n\u001b[0;32m      7\u001b[0m     sample_mode \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mrandom\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m     start_learn_iterations \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, \n\u001b[0;32m      9\u001b[0m     bootstrap_iterations \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m, \n\u001b[0;32m     10\u001b[0m     bootstrap_every\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[11], line 138\u001b[0m, in \u001b[0;36mdqn\u001b[1;34m(n_episodes, eps_start, eps_end, eps_decay, step_penalty, sample_mode, start_learn_iterations, bootstrap_iterations, bootstrap_every)\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[39mbreak\u001b[39;00m \n\u001b[0;32m    137\u001b[0m \u001b[39m# Do the actual learning\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m agent\u001b[39m.\u001b[39;49mlearn(learn_iterations, mode\u001b[39m=\u001b[39;49msample_mode, save_loss \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, weight \u001b[39m=\u001b[39;49m env\u001b[39m.\u001b[39;49mgame_board\u001b[39m.\u001b[39;49mmax())\n\u001b[0;32m    140\u001b[0m \u001b[39m# Calculate action stats\u001b[39;00m\n\u001b[0;32m    141\u001b[0m actions \u001b[39m=\u001b[39m actions \u001b[39m/\u001b[39m env\u001b[39m.\u001b[39msteps\n",
      "File \u001b[1;32mc:\\Users\\lenan\\FRI\\2048\\AIproject\\3_attempt\\agent.py:338\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self, learn_iterations, mode, save_loss, gamma, weight)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[39m# Minimize the loss\u001b[39;00m\n\u001b[0;32m    337\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> 338\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    339\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    341\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_decay\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\lenan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lenan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the training\n",
    "dqn(n_episodes=100000, \n",
    "    eps_start=eps or 0.05, \n",
    "    eps_end=0.00001, \n",
    "    eps_decay=0.999,\n",
    "    step_penalty = 0, \n",
    "    sample_mode = 'random',\n",
    "    start_learn_iterations = 10, \n",
    "    bootstrap_iterations = 0, \n",
    "    bootstrap_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
